{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS667: Practical Data Science - Fall 2025  \n",
        "## A#2 Draft  \n",
        "Authors: Michael Griffin, Ramakrishna Sonakam, Will Torres  "
      ],
      "metadata": {
        "id": "BhM6UW4uoWk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Ik1zqhZ2nSdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "59krpdCPnWLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_complaints = pd.read_csv('/content/drive/MyDrive/PDS/data/complaints.csv')\n",
        "df_stalled = pd.read_csv('/content/drive/MyDrive/PDS/data/stalled.csv')"
      ],
      "metadata": {
        "id": "jv3b0c3anYqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect Data"
      ],
      "metadata": {
        "id": "nISf2nAynd2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_complaints.head()"
      ],
      "metadata": {
        "id": "bxowx7j1nf-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stalled.head()"
      ],
      "metadata": {
        "id": "6iY27lXinjXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def inspect_dataframe(df):\n",
        "    print(\"\\nData types:\")\n",
        "    print(df.info())\n",
        "\n",
        "    print(\"\\nDescriptive statistics for numerical columns:\")\n",
        "    display(df.describe())\n",
        "\n",
        "    # mg/rs - add more checks here..."
      ],
      "metadata": {
        "id": "7nPgygmtnrGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inspect_dataframe(df_complaints)"
      ],
      "metadata": {
        "id": "9fLIRhHansuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inspect_dataframe(df_stalled)"
      ],
      "metadata": {
        "id": "R1oxcyLFntW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stalled['complaint_number'] = df_stalled['complaint_number'].astype('str')\n",
        "df_complaints['complaint_number'] = df_complaints['complaint_number'].astype('str')"
      ],
      "metadata": {
        "id": "_0YffzGqnvix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Priority to Category"
      ],
      "metadata": {
        "id": "VYRQgvBCpojl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "complaint_priority_map = {\n",
        "    \"03\":\"A\",\"04\":\"A\",\"05\":\"B\",\"06\":\"B\",\"09\":\"B\",\"10\":\"A\",\"12\":\"A\",\"13\":\"A\",\n",
        "    \"14\":\"A\",\"15\":\"A\",\"16\":\"B\",\"18\":\"A\",\"20\":\"A\",\"21\":\"B\",\"23\":\"B\",\"29\":\"B\",\n",
        "    \"30\":\"C\",\"31\":\"A\",\"35\":\"C\",\"37\":\"A\",\"45\":\"B\",\"49\":\"C\",\"50\":\"A\",\"52\":\"B\",\n",
        "    \"53\":\"D\",\"54\":\"B\",\"55\":\"D\",\"56\":\"A\",\"58\":\"B\",\"59\":\"B\",\"62\":\"A\",\"63\":\"B\",\n",
        "    \"65\":\"A\",\"66\":\"B\",\"67\":\"A\",\"71\":\"B\",\"73\":\"C\",\"74\":\"C\",\"75\":\"B\",\"76\":\"A\",\n",
        "    \"77\":\"C\",\"78\":\"B\",\"79\":\"C\",\"80\":\"D\",\"81\":\"A\",\"82\":\"A\",\"83\":\"B\",\"85\":\"C\",\n",
        "    \"86\":\"A\",\"88\":\"B\",\"89\":\"A\",\"90\":\"C\",\"91\":\"A\",\"92\":\"B\",\"93\":\"B\",\"94\":\"C\",\n",
        "    \"1A\":\"B\",\"1B\":\"B\",\"1D\":\"B\",\"1E\":\"A\",\"1G\":\"B\",\"1K\":\"D\",\"1Z\":\"D\",\"2A\":\"B\",\n",
        "    \"2B\":\"A\",\"2C\":\"B\",\"2D\":\"B\",\"2E\":\"A\",\"2F\":\"D\",\"2G\":\"C\",\"2H\":\"D\",\"2J\":\"D\",\n",
        "    \"2K\":\"D\",\"2L\":\"D\",\"2M\":\"D\",\"3A\":\"B\",\"4A\":\"B\",\"4B\":\"B\",\"4C\":\"D\",\"4D\":\"D\",\n",
        "    \"4F\":\"D\",\"4G\":\"B\",\"4J\":\"D\",\"4K\":\"D\",\"4L\":\"D\",\"4M\":\"D\",\"4N\":\"D\",\"4P\":\"C\",\n",
        "    \"4W\":\"C\",\"5A\":\"B\",\"5B\":\"A\",\"5C\":\"A\",\"5E\":\"A\",\"5F\":\"B\",\"5G\":\"B\",\"6A\":\"C\"\n",
        "}\n",
        "\n",
        "df_complaints['priority'] = df_complaints['complaint_category'].map(complaint_priority_map)\n"
      ],
      "metadata": {
        "id": "mSuQ-__5pXqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduce Dimensions of Disposition and Complaint Categories\n",
        "- Label each feature to reduce into 'bins'"
      ],
      "metadata": {
        "id": "Ut1m8w2URZ42"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dL5Pbm2vRYrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging Datasets"
      ],
      "metadata": {
        "id": "Ml_h-b0cpYIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate complaint numbers in each dataset\n",
        "complaints_dupes = df_complaints['complaint_number'].duplicated().sum()\n",
        "stalled_dupes = df_stalled['complaint_number'].duplicated().sum()\n",
        "\n",
        "# Check unique complaint numbers\n",
        "unique_complaints_df_complaints = df_complaints['complaint_number'].nunique()\n",
        "unique_complaints_df_stalled = df_stalled['complaint_number'].nunique()\n",
        "\n",
        "# Check for overlaping complaints\n",
        "overlap = set(df_complaints['complaint_number']) & set(df_stalled['complaint_number'])\n",
        "overlap_count = len(overlap)\n",
        "\n",
        "print(f\"Complaints Dataset: {len(df_complaints)} rows, {unique_complaints_df_complaints} unique complaints\")\n",
        "print(f\"Stalled Dataset: {len(df_stalled)} rows, {unique_complaints_df_stalled} unique complaints\")\n",
        "print(f\"Overlapping complaints: {overlap_count}\")\n",
        "print(f\"Duplication rate - Complaints: {complaints_dupes/len(df_complaints):.2%}\")\n",
        "print(f\"Duplication rate - Stalled: {stalled_dupes/len(df_stalled):.2%}\")"
      ],
      "metadata": {
        "id": "dwGeA0wIqlDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deduplicate datasets\n",
        "df_complaints_clean = (df_complaints.sort_values('dobrundate', ascending=False)\n",
        "             .drop_duplicates(subset='complaint_number', keep='first'))\n",
        "\n",
        "df_stalled_clean = (df_stalled.sort_values('dobrundate', ascending=False)\n",
        "             .drop_duplicates(subset='complaint_number', keep='first'))\n",
        "\n",
        "print(f\"Complaints after deduplication: {len(df_complaints_clean)} (removed {len(df_complaints) - len(df_complaints_clean)})\")\n",
        "print(f\"Stalled after deduplication: {len(df_stalled_clean)} (removed {len(df_stalled) - len(df_stalled_clean)})\")"
      ],
      "metadata": {
        "id": "QeJJ4GV8skzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IF WE ARE PRESERVING ALL DATA:\n",
        "merged = df_complaints_clean.merge(\n",
        "    df_stalled_clean[['complaint_number', 'date_complaint_received', 'dobrundate']],\n",
        "    on='complaint_number',\n",
        "    how='left',\n",
        "    suffixes=('', '_stalled'),\n",
        "    indicator=True  # to track source\n",
        ")\n",
        "\n",
        "# Create stalled indicator\n",
        "merged['is_stalled'] = (merged['_merge'] == 'both').astype(int)\n",
        "\n",
        "print(f\"Total rows after merge: {len(merged)}\")\n",
        "print(merged['_merge'].value_counts())\n",
        "# left_only = complaints without stalled status\n",
        "# right_only = stalled sites without complaint details (data quality issue?)\n",
        "# both = complete records\n",
        "\n",
        "# Note: there is probably another/more efficient way to merge the data?"
      ],
      "metadata": {
        "id": "dhIXOvq2s2Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged.head(10)"
      ],
      "metadata": {
        "id": "Cli-P_FeuEx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inspect_dataframe(merged)"
      ],
      "metadata": {
        "id": "rjDHkTvR_j-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date columns\n",
        "date_cols = ['date_entered', 'disposition_date', 'inspection_date', 'date_complaint_received']\n",
        "\n",
        "for col in date_cols:\n",
        "    if col in merged.columns:\n",
        "        merged[col] = pd.to_datetime(merged[col], errors='coerce')"
      ],
      "metadata": {
        "id": "-iWkfrnY2XHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "gWdn0pzP4-Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stalled_df = merged[merged['is_stalled'] == 1].copy()\n",
        "stalled_df['date_complaint_received'] = pd.to_datetime(stalled_df['date_complaint_received'], errors='coerce')\n",
        "stalled_df['year_month'] = stalled_df['date_complaint_received'].dt.to_period('M')\n",
        "stalled_monthly_counts = stalled_df['year_month'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "stalled_monthly_counts.plot(kind='line', marker='o', linestyle='-')\n",
        "plt.title('Stalled Complaint Volume Over Time (Monthly)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Date', fontsize=10)\n",
        "plt.ylabel('Count', fontsize=10)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZEGItZhQymuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Visualizations:\n",
        "- kde on map of zip_code\n",
        "- disposition code\n",
        "'''"
      ],
      "metadata": {
        "id": "SKiPBcg_BYgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bar of status\n",
        "status_counts = merged['status'].value_counts()\n",
        "plt.figure(figsize=(10, 6))\n",
        "status_counts.plot(kind='bar')\n",
        "plt.title('Status Counts')\n",
        "plt.xlabel('Status')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Active complaints: {status_counts['ACTIVE']}\")"
      ],
      "metadata": {
        "id": "VkTFkZrqCZf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top 10 complaint categories per priority label\n",
        "priority_levels = ['A', 'B', 'C', 'D']\n",
        "\n",
        "priority_colors = {\n",
        "    'A': '#1f77b4',  # Blue\n",
        "    'B': '#ff7f0e',  # Orange\n",
        "    'C': '#2ca02c',  # Green\n",
        "    'D': '#d62728'   # Red\n",
        "}\n",
        "\n",
        "complaints = merged.dropna(subset=['priority'])\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, priority in enumerate(priority_levels):\n",
        "    ax = axes[i]\n",
        "    subset = complaints[complaints['priority'] == priority]\n",
        "\n",
        "    top_categories = subset['complaint_category'].value_counts().nlargest(10).index\n",
        "    top_subset = subset[subset['complaint_category'].isin(top_categories)]\n",
        "\n",
        "    sns.countplot(\n",
        "        data=top_subset,\n",
        "        x='complaint_category',\n",
        "        order=top_categories,\n",
        "        ax=ax,\n",
        "        color=priority_colors[priority]\n",
        "    )\n",
        "\n",
        "    ax.set_title(f'Top 10 Complaint Categories â€” Priority {priority}', fontsize=14)\n",
        "    ax.set_xlabel('Complaint Category')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J5JbAreJ4pbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Transformation / Engineering\n",
        "\n",
        "- **Classification** Idea: Can we flag construction sites likely to stall within the next 90 days?\n",
        "  - Requires us to calculate a target feature based on historical data - i.e., determining sites that have stalled within 90D(?) of a given date\n",
        "  - Engineered Features\n",
        "    - `complaints_last_30_days` = Count of complaints in past 30 days  \n",
        "    - `complaints_last_90_days` = Count of complaints in past 90 days  \n",
        "    - `complaint_ROI` = complaints_last_30 / complaints_last_90 (rate of increase)  \n",
        "    - `recent_priority_a_count` = Priority A complaints in last 60 days  \n",
        "    - `days_since_last_complaint` = Current date - max(date_entered)  \n",
        "    - `unresolved_complaints_current` = Count with no disposition as of prediction date  \n",
        "    - `recent_stop_work_orders` = Stop work dispositions in last 90 days  \n",
        "    - `inspection_frequency_recent` = Inspections in last 60 days  \n",
        "    - `resolution_time_increasing` = Binary: avg resolution time last 60d > previous 60d\n",
        "    - `total_complaints_to_date` = Count of all complaints up to prediction point  \n",
        "    - `unique_categories_to_date` = Unique complaint categories to date  \n",
        "    - `has_structural_complaints` = Ever had categories 12, 14, 30, 55, ...  \n",
        "    - `has_permit_violations` = Ever had categories 05, 06, 66, ...\n",
        "    - `has_illegal_work` = Ever had categories 76, 3A, 5G, ...\n",
        "    - ... other metrics (maybe related to category patterns?)\n",
        "\n",
        "- **Regression** Idea: Can we identify how long a stalled construction (in days) will remain inactive?\n",
        "  - Transformed / Normalized Features\n",
        "    - `date_entered` = datetime; extract: year, month, day_of_week, quarter  \n",
        "    - `date_complaint_received` = datetime  \n",
        "    - `disposition_date` = datetime  \n",
        "    - `inspection_date` = datetime  \n",
        "    - `complaint_category` = Map to priority level (A/B/C/D)  \n",
        "    - `status` = Categorical encoding (Active/Closed/...other?)  \n",
        "    - `disposition_code` = Group into outcome categories (violations/stop work/cleared)  \n",
        "    - `borough` = Extract from complaint_number first digit  \n",
        "    - `community_board` = Separate into borough_code and board_number\n",
        "  - Engineered Features\n",
        "    - **TARGET**: `stall_duration_days` = current_date - date_complaint_received (or last activity date?)  \n",
        "    - `complaints_before_stall` = Count of complaints before stall date per BIN  \n",
        "    - `time_to_stall` = date_complaint_received - min(date_entered) per BIN  \n",
        "    - `avg_resolution_time_before_stall` = Mean(disposition_date - date_entered) per BIN  \n",
        "    - `unresolved_complaints_at_stall` = Count where disposition_date is null at stall time  \n",
        "    - `priority_a_ratio` = Priority A complaints / total complaints per BIN  \n",
        "    - `complaint_category_diversity` = Number of unique complaint categories per BIN  \n",
        "    - `stop_work_orders_issued` = Count of disposition codes indicating stop work  \n",
        "    - `inspection_frequency` = Count of inspections / time active before stall  \n",
        "    - `last_activity_days` = date_complaint_received - max(disposition_date, inspection_date)  \n",
        "    - `violation_escalation_rate` = Change in complaint severity over time  \n",
        "    - `seasonal_stall_indicator` = Month/quarter when stalled (construction seasonality)  \n",
        "    - `days_first_to_last_complaint` = max(date_entered) - min(date_entered) per BIN  \n",
        "    - `complaint_velocity` = complaints_before_stall / days_first_to_last_complaint\n",
        "\n"
      ],
      "metadata": {
        "id": "RJMUIKN-O3sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we want to embed/encode the data as it is mostly categorical/temporal?"
      ],
      "metadata": {
        "id": "IMAHa4bUCBiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder as le\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# draft encoding\n",
        "df = merged[[\n",
        "    'status','zip_code','bin','community_board','complaint_category',\n",
        "    'unit','disposition_date','disposition_code','inspection_date',\n",
        "    'priority','is_stalled'\n",
        "]].copy()\n",
        "\n",
        "for col in ['disposition_date', 'inspection_date']:\n",
        "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "    df[col] = df[col].view('int64')\n",
        "\n",
        "priority_map = {'A': 4, 'B': 3, 'C': 2, 'D': 1}\n",
        "df['priority'] = df['priority'].map(priority_map)\n",
        "\n",
        "df['community_board'] = df['community_board'].astype(str)\n",
        "df['zip_code'] = df['zip_code'].astype(str)\n",
        "\n",
        "cat_nominal = ['status','zip_code','community_board','complaint_category','unit','disposition_code']\n",
        "df[cat_nominal] = df[cat_nominal].fillna('Unknown')\n",
        "\n",
        "num_cols = ['bin','disposition_date','inspection_date','priority']\n",
        "for c in num_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "df[num_cols] = df[num_cols].fillna(-1)\n"
      ],
      "metadata": {
        "id": "htS9qg6I7jHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for tree models -- ordinal/label-like encoding for nominal categoricals\n",
        "tree_pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_nominal),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# convert ord transformation into dataframe for debugging purposes\n",
        "X_tree = tree_pre.fit_transform(df.drop(columns=['is_stalled']))\n",
        "ord_cols = cat_nominal + [c for c in df.drop(columns=['is_stalled']) if c not in cat_nominal]\n",
        "X_tree_df = pd.DataFrame(X_tree, columns=ord_cols)"
      ],
      "metadata": {
        "id": "C4dMaadvHvd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation\n",
        "df_corr = X_tree_df.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df_corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nd_LQpUITDvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for linear reg models -- One-Hot for nominal categoricals\n",
        "lin_pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('ohe', OneHotEncoder(handle_unknown='ignore', drop='first'), cat_nominal),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# convert ohe transformation into dataframe for debugging purposes -- ### Note: CRASHES (USES TOO MUCH RAM)\n",
        "X_linear = lin_pre.fit_transform(df.drop(columns=['is_stalled']))\n",
        "\n",
        "ohe = lin_pre.named_transformers_['ohe']\n",
        "ohe_cols = ohe.get_feature_names_out(cat_nominal)\n",
        "\n",
        "num_cols = [c for c in df.drop(columns=['is_stalled']) if c not in cat_nominal]\n",
        "\n",
        "lin_cols = list(ohe_cols) + num_cols\n",
        "X_linear_df = pd.DataFrame(X_linear.toarray(), columns=lin_cols)"
      ],
      "metadata": {
        "id": "L52SwgAZHxl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3e4zZa5-O2tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Normalization/Standardization"
      ],
      "metadata": {
        "id": "3qntKq6jSONQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0PecL03SNo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling\n",
        "\n",
        "Classification:\n",
        "- Logistic, KNN, Naive Bayes\n",
        "- RF, XGB, Light GB\n",
        "\n",
        "Regression:\n",
        "- Linear, ARIMA\n",
        "- Lasso, Ridge, SVR (Polynomial, RBF) -- if applicable"
      ],
      "metadata": {
        "id": "gJGyXcMRSS_t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YLtKI6XLSv1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}